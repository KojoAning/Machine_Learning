{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the paths for your audio files\n",
    "LJSpeech = r\"C:\\Users\\ANING\\Downloads\\LJSpeech300\"\n",
    "CVcorpus = r\"C:\\Users\\ANING\\Downloads\\CVcorpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the folder to store your various spectrum images\n",
    "Spectr_image = r\"C:\\Users\\ANING\\Downloads\\Automatic-Voice-Recognition-main\\Spectrum_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create folders for the different audio files\n",
    "os.makedirs('Spectrum_images\\CVcorpus')\n",
    "os.makedirs('Spectrum_images\\LJSpeech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#function to process audio and generate the spectrum images\n",
    "def process_audio(audio_path,save_location,filename):   \n",
    "    audio, fs = librosa.load(audio_path)\n",
    "    D = librosa.amplitude_to_db(librosa.stft(audio), ref=np.max)\n",
    "    # Save the spectrogram\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.axis('off')\n",
    "    librosa.display.specshow(D, sr=fs, x_axis='time', y_axis='linear',cmap='viridis')\n",
    "    plt.savefig(save_location+'\\\\'+filename+'.jpg', dpi=300, bbox_inches='tight', pad_inches=0,transparent=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function \n",
    "def crop_image(image):\n",
    "    imag = np.array(image)\n",
    "    # print(imag)\n",
    "    mask = np.zeros(image.shape,dtype=np.uint8)\n",
    "    cv.rectangle(mask,(100,50),(710,585),(255,255,255),-1)\n",
    "    image = np.bitwise_and(imag,mask)\n",
    "    x, y, w, h = 100,50,610,535\n",
    "\n",
    "    # Crop the image using the bounding rectangle coordinates\n",
    "    cropped_image = image[y:y + h, x:x + w]\n",
    "    image =np.array(image)\n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process the audio files to generate the spectrum images\n",
    "for audios in os.listdir(CVcorpus):\n",
    "    process_audio(CVcorpus+f'\\\\{audios}',r\"C:\\Users\\ANING\\Downloads\\Automatic-Voice-Recognition-main\\Spectrum_images\\CVcorpus\",audios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process the audio files to generate the spectrum images\n",
    "for audios in os.listdir(LJSpeech):\n",
    "    process_audio(LJSpeech+f'\\\\{audios}',r'C:\\Users\\ANING\\Downloads\\Automatic-Voice-Recognition-main\\Spectrum_images\\LJSpeech',audios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 601 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_images = image_datagen.flow_from_directory(Spectr_image,target_size=(300,300),class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.applications.vgg16.VGG16(include_top=False,weights='imagenet',input_shape=(300,300,3)))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(512,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(32,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 9, 9, 512)         14714688  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 41472)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               21234176  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 35,949,377\n",
      "Trainable params: 35,949,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "19/19 [==============================] - 429s 23s/step - loss: 0.9572 - accuracy: 0.6988\n",
      "Epoch 2/10\n",
      "19/19 [==============================] - 606s 32s/step - loss: 0.3365 - accuracy: 0.8819\n",
      "Epoch 3/10\n",
      "19/19 [==============================] - 496s 26s/step - loss: 0.1587 - accuracy: 0.9468\n",
      "Epoch 4/10\n",
      "19/19 [==============================] - 486s 26s/step - loss: 0.0487 - accuracy: 0.9917\n",
      "Epoch 5/10\n",
      "19/19 [==============================] - 436s 23s/step - loss: 0.0224 - accuracy: 0.9933\n",
      "Epoch 6/10\n",
      "19/19 [==============================] - 455s 24s/step - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "19/19 [==============================] - 519s 28s/step - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "19/19 [==============================] - 457s 24s/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "19/19 [==============================] - 453s 24s/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "19/19 [==============================] - 454s 24s/step - loss: 0.0025 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2175ef62460>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images,epochs=10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('final_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
